from youtube_comment_downloader import YoutubeCommentDownloader
from pymongo import MongoClient
from datetime import datetime, timezone
import requests
import re
from app.core.config import settings

API_KEY = settings.youtube_api_key

# MongoDB ì—°ê²°
client = MongoClient(settings.mongo_uri)
db = client["kickon"]
collection = db["youtube_comments"]

# ì˜ìƒ ê²€ìƒ‰ (ì—…ë¡œë“œì¼ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§)
def search_videos(query, max_results=5):
    search_url = "https://www.googleapis.com/youtube/v3/search"
    params = {
        "part": "snippet",
        "q": query,
        "type": "video",
        "maxResults": max_results,
        "order": "date",  # ìµœì‹ ìˆœ ì •ë ¬
        "key": API_KEY
    }
    response = requests.get(search_url, params=params)
    response.raise_for_status()
    items = response.json()["items"]

    today = datetime.now(timezone.utc).date() 

    return [
        {
            "video_id": item["id"]["videoId"],
            "title": item["snippet"]["title"],
            "published_at": item["snippet"]["publishedAt"]
        }
        for item in items
        #if datetime.strptime(item["snippet"]["publishedAt"], "%Y-%m-%dT%H:%M:%SZ").date() == today
    ]

# ëŒ“ê¸€ í¬ë¡¤ë§ ë° ì €ì¥
def crawl_and_store_comments(video_id, video_title, published_at, query=None):
    downloader = YoutubeCommentDownloader()
    video_url = f"https://www.youtube.com/watch?v={video_id}"

    print(f"[í¬ë¡¤ë§ ì‹œì‘] {video_title} ({video_id})")

    comment_data = []
    try:
        for comment in downloader.get_comments_from_url(video_url, sort_by=0):  # ì¸ê¸°ìˆœ
            comment_text = comment["text"]
            comment_obj = {
                "author": comment.get("author", "unknown"),
                "text": comment_text,
                "published_at": datetime.utcnow(),
                "text_for_embedding": f"ì˜ìƒ ì œëª©: {video_title}\nëŒ“ê¸€: {comment_text}"
            }
            comment_data.append(comment_obj)
    except Exception as e:
        print(f"[ì—ëŸ¬ ë°œìƒ] {e}")
        return

    doc = {
        "video_id": video_id,
        "video_title": video_title,
        "video_url": video_url,
        "team_mentioned": query,
        "match_date": None,
        "video_published_at": published_at,
        "crawled_at": datetime.utcnow(),
        "comments": comment_data
    }

    collection.insert_one(doc)
    print(f"[ì €ì¥ ì™„ë£Œ] ëŒ“ê¸€ {len(comment_data)}ê°œ ì €ì¥")

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    teams = [("ìˆ˜ì› ì‚¼ì„±", "FC ì„œìš¸"), ("ì¸ì²œ", "ëŒ€ì „")]
    queries = [f"{home} vs {away} í•˜ì´ë¼ì´íŠ¸" for home, away in teams]

    for query in queries:
        print(f"\nğŸ” ê²€ìƒ‰ì–´: {query}")
        videos = search_videos(query)
        for video in videos:
            video_id = video["video_id"]

            # ì¤‘ë³µ ë°©ì§€
            if collection.find_one({"video_id": video_id}):
                print(f"â© ì´ë¯¸ í¬ë¡¤ë§í•œ ì˜ìƒì…ë‹ˆë‹¤: {video['title']}")
                continue

            print("ğŸ“º", video["title"])
            crawl_and_store_comments(video["video_id"], video["title"], video["published_at"], query=query)