import requests
from bs4 import BeautifulSoup
from time import sleep
from typing import Optional
from datetime import datetime, timezone
import re
import random
import time
import cloudscraper
import sys

from app.services.mongo_utils import save_fmkorea_post_doc

BASE_URL = "https://www.fmkorea.com"

BOARD_PATHS = {
    "í•´ì™¸ì¶•êµ¬": "football_world",
    "êµ­ë‚´ì¶•êµ¬": "football_korean",
    "ì¶•êµ¬ì†Œì‹í†µ": "football_news"
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
    "Referer": "https://www.fmkorea.com/",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}

scraper = cloudscraper.create_scraper()

# ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘
def get_post_links(scraper, board_path: str, page: int = 1) -> list[str]:
    url = f"{BASE_URL}/{board_path}?page={page}"
    res = scraper.get(url, headers=HEADERS)
    text_preview = res.text[:1000] 

    # ì°¨ë‹¨ ì—¬ë¶€ ê°ì§€
    if (
        res.status_code == 403
        or "Cloudflare" in res.text
        or "Please enable JavaScript" in res.text
        or re.search(r'var\s+[a-zA-Z]{3}\s*=\s*"";', text_preview)  # e.g., var RTE = "";
        or re.search(r'\["[a-zA-Z0-9+/=]+",\s*"[a-zA-Z0-9+/=]+"\]', text_preview)  # base64-like ë°°ì—´
    ):
        print("ğŸš« í¬ë¡¤ë§ ì°¨ë‹¨ë¨! Cloudflare ë˜ëŠ” ë´‡ ì°¨ë‹¨ í˜ì´ì§€ ê°ì§€ë¨.")
        sys.exit(1)

    soup = BeautifulSoup(res.text, "html.parser")

    links = []
    for tr in soup.select("tr"):
         # ê³µì§€ê¸€ ê±´ë„ˆëœ€
        if "notice" in tr.get("class", []):
            continue

        a_tag = tr.select_one("td.title a[href]")
        if not a_tag:
            continue
        href = a_tag.get("href")
        if href.startswith("/") and not href.startswith("http"):
            full_url = BASE_URL + href
            links.append(full_url)

    print(f"ğŸ”— ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ ({len(links)}ê°œ): {url}")
    return links

# ê²Œì‹œê¸€ ë³¸ë¬¸ + ëŒ“ê¸€ ì¶”ì¶œ
def parse_post(scraper, url: str) -> Optional[dict]:
    res = scraper.get(url, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")

    title_tag = soup.select_one("h1 .np_18px_span") or soup.select_one("h1 span")
    content_tag = soup.select_one("div.xe_content") or soup.select_one("div.rd_body")

    if not title_tag:
        print(f"â­ï¸ ê±´ë„ˆëœ€ (ì œëª© ì—†ìŒ): {url}")
        return None

    title = title_tag.get_text(strip=True)
    content = content_tag.get_text(strip=True) if content_tag else ""

    # ë‚ ì§œ
    date_tag = soup.select_one("span.date") or soup.select_one("span.rd_time")
    date_str = date_tag.get_text(strip=True) if date_tag else None

    try:
        post_datetime = datetime.strptime(date_str, "%Y.%m.%d %H:%M")
        post_datetime = post_datetime.replace(tzinfo=timezone.utc)
    except:
        print(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str}")
        post_datetime = None

    # ì¡°íšŒìˆ˜
    view_tag = soup.select_one("span.hit") or soup.select_one("span.rd_view")
    try:
        view_count = int(view_tag.get_text(strip=True).replace(",", "")) if view_tag else None
    except:
        print(f"âš ï¸ ì¡°íšŒìˆ˜ íŒŒì‹± ì‹¤íŒ¨: {view_tag}")
        view_count = None

    # ì¶”ì²œ ìˆ˜
    like_tag = soup.select_one("span.vote") or soup.select_one("span#good_button")
    like_text = like_tag.get_text(strip=True) if like_tag else "0"
    match = re.search(r'\d+', like_text)
    like_count = int(match.group()) if match else 0

    # ëŒ“ê¸€ ì¶”ì¶œ
    comments = []
    comment_tags = soup.select("div.comment-content")
    for tag in comment_tags:
        comment = tag.get_text(strip=True)
        if comment:
            comments.append(comment)

    return {
        "url": url,
        "title": title,
        "content": content,
        "comments": comments,
        "source": "fmkorea",
        "crawled_at": datetime.now(timezone.utc),
        "post_datetime": post_datetime,
        "view_count": view_count,
        "like_count": like_count,
        "text_for_embedding": f"{title}: {content}"
    }


# ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹¤í–‰
def crawl_fmkorea_board(board_name: str, page_limit: int = 1):
    board_path = BOARD_PATHS.get(board_name)
    if not board_path:
        print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ê²Œì‹œíŒ ì´ë¦„: {board_name}")
        return

    session = requests.Session()

    for page in range(1, page_limit + 1):
        print(f"\nğŸ“„ [{board_name}] {page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
        post_links = get_post_links(scraper, board_path, page)

        for idx, url in enumerate(post_links):
            # # ì´ë¯¸ DBì— ì €ì¥ëœ ê²Œì‹œê¸€ì´ë©´ ì¤‘ë‹¨
            # if fmkorea_collection.find_one({"url": url}):
            #     print(f"ğŸ›‘ ì´ë¯¸ ìˆ˜ì§‘í•œ ê²Œì‹œê¸€ ë°œê²¬ â†’ í¬ë¡¤ë§ ì¤‘ë‹¨")
            #     return

            # print(f"\nâ¡ï¸ [{idx+1}/{len(post_links)}] í¬ë¡¤ë§ ì¤‘: {url}")
            try:
                post = parse_post(scraper, url)
                if post:
                    save_fmkorea_post_doc(post)
                    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {post['title'][:30]}")
                else:
                    print(f"âŒ ì €ì¥ ìŠ¤í‚µ (íŒŒì‹± ì‹¤íŒ¨): {url}")
            except Exception as e:
                print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {e} â†’ {url}")
            
            delay = random.uniform(1.0, 2.5)  # 1.0ì´ˆ ~ 2.5ì´ˆ ì‚¬ì´ ëœë¤
            time.sleep(delay)

# ì˜ˆì‹œ ì‹¤í–‰
if __name__ == "__main__":
    crawl_fmkorea_board("í•´ì™¸ì¶•êµ¬", page_limit=5)